{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nasty-packaging",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T14:10:59.918360Z",
     "start_time": "2021-02-06T14:10:58.600208Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "synthetic-washer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T12:22:25.065851Z",
     "start_time": "2021-02-06T12:22:20.869506Z"
    }
   },
   "outputs": [],
   "source": [
    "rooth_path = '../rawdata/proc_data/'\n",
    "df_train_gearbox = pd.read_csv(rooth_path + 'df_train_gearbox.csv')\n",
    "df_train_gen = pd.read_csv(rooth_path + 'df_train_gen.csv')\n",
    "df_train_gen_bear = pd.read_csv(rooth_path + 'df_train_gen_bear.csv')\n",
    "df_train_hyd = pd.read_csv(rooth_path + 'df_train_hyd.csv')\n",
    "df_train_transf = pd.read_csv(rooth_path + 'df_train_transf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "personal-signature",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T12:22:25.883567Z",
     "start_time": "2021-02-06T12:22:25.070647Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_gen = pd.read_csv(rooth_path + 'df_test_gen.csv')\n",
    "df_test_gearbox = pd.read_csv(rooth_path + 'df_test_gearbox.csv')\n",
    "df_test_gen_bear = pd.read_csv(rooth_path + 'df_test_gen_bear.csv')\n",
    "df_test_hyd = pd.read_csv(rooth_path + 'df_test_hyd.csv')\n",
    "df_test_transf = pd.read_csv(rooth_path + 'df_test_transf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "acquired-bandwidth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T12:22:26.723258Z",
     "start_time": "2021-02-06T12:22:25.886234Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_gearbox['Timestamp'] = pd.to_datetime(df_train_gearbox['Timestamp'])\n",
    "df_train_gen['Timestamp'] = pd.to_datetime(df_train_gen['Timestamp'])\n",
    "df_train_gen_bear['Timestamp'] = pd.to_datetime(df_train_gen_bear['Timestamp'])\n",
    "df_train_hyd['Timestamp'] = pd.to_datetime(df_train_hyd['Timestamp'])\n",
    "df_train_transf['Timestamp'] = pd.to_datetime(df_train_transf['Timestamp'])\n",
    "df_test_gearbox['Timestamp'] = pd.to_datetime(df_test_gearbox['Timestamp'])\n",
    "df_test_gen['Timestamp'] = pd.to_datetime(df_test_gen['Timestamp'])\n",
    "df_test_gen_bear['Timestamp'] = pd.to_datetime(df_test_gen_bear['Timestamp'])\n",
    "df_test_hyd['Timestamp'] = pd.to_datetime(df_test_hyd['Timestamp'])\n",
    "df_test_transf['Timestamp'] = pd.to_datetime(df_test_transf['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "better-perspective",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T12:22:26.734709Z",
     "start_time": "2021-02-06T12:22:26.725769Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_features(df_in, rolling_win_size):\n",
    "    \"\"\"Add rolling average and rolling standard deviation for sensors readings using fixed rolling window size.\n",
    "    Args:\n",
    "            df_in (dataframe)     : The input dataframe to be proccessed (training or test)\n",
    "            rolling_win_size (int): The window size, number of cycles for applying the rolling function\n",
    "    Returns:\n",
    "            dataframe: contains the input dataframe with additional rolling mean and std for each sensor\n",
    "    \"\"\"\n",
    "    \n",
    "    sensor_cols = []\n",
    "    for i in df_in.keys()[2:-5]:\n",
    "        sensor_cols.append(i)\n",
    "    sensor_av_cols = [nm+'_av' for nm in sensor_cols]\n",
    "    sensor_sd_cols = [nm+'_sd' for nm in sensor_cols]\n",
    "    df_out = pd.DataFrame()\n",
    "    ws = rolling_win_size\n",
    "    #calculate rolling stats for each engine id\n",
    "    for m_id in pd.unique(df_in.Turbine_ID):\n",
    "        # get a subset for each engine sensors\n",
    "        df_engine = df_in[df_in['Turbine_ID'] == m_id]\n",
    "        df_sub = df_engine[sensor_cols]\n",
    "        # get rolling mean for the subset\n",
    "        av = df_sub.rolling(ws, min_periods=1).mean()\n",
    "        av.columns = sensor_av_cols\n",
    "        # get the rolling standard deviation for the subset\n",
    "        sd = df_sub.rolling(ws, min_periods=1).std().fillna(0)\n",
    "        sd.columns = sensor_sd_cols\n",
    "        # combine the two new subset dataframes columns to the engine subset\n",
    "        new_ftrs = pd.concat([df_engine,av,sd], axis=1)\n",
    "        # add the new features rows to the output dataframe\n",
    "        df_out = pd.concat([df_out,new_ftrs])\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "developing-namibia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T12:22:33.234515Z",
     "start_time": "2021-02-06T12:22:26.741071Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_gearbox_extra = add_features(df_train_gearbox, 15)\n",
    "df_train_gen_extra = add_features(df_train_gen, 15)\n",
    "df_train_gen_bear_extra = add_features(df_train_gen_bear, 15)\n",
    "df_train_hyd_extra = add_features(df_train_hyd, 15)\n",
    "df_train_transf_extra = add_features(df_train_transf, 15)\n",
    "df_test_gearbox_extra = add_features(df_test_gearbox, 15)\n",
    "df_test_gen_extra = add_features(df_test_gen, 15)\n",
    "df_test_gen_bear_extra = add_features(df_test_gen_bear, 15)\n",
    "df_test_hyd_extra = add_features(df_test_hyd, 15)\n",
    "df_test_transf_extra = add_features(df_test_transf, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "continuous-huntington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T12:22:33.242509Z",
     "start_time": "2021-02-06T12:22:33.236806Z"
    }
   },
   "outputs": [],
   "source": [
    "#Group by day per turbine\n",
    "def group_per_frequency(df, strategy='mean'):\n",
    "    df['Date'] = df['Timestamp'].dt.date\n",
    "    if strategy == 'max':\n",
    "        df = df.groupby(by=['Turbine_ID','Date']).max().reset_index().drop(columns='Timestamp')\n",
    "    else:\n",
    "        df = df.groupby(by=['Turbine_ID','Date']).mean().reset_index()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "emerging-wildlife",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T13:02:07.092623Z",
     "start_time": "2021-02-06T13:01:59.853571Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_gearbox_day = group_per_frequency(df_train_gearbox_extra)\n",
    "df_train_gen_day = group_per_frequency(df_train_gen_extra)\n",
    "df_train_gen_bear_day = group_per_frequency(df_train_gen_bear_extra)\n",
    "df_train_hyd_day = group_per_frequency(df_train_hyd_extra)\n",
    "df_train_transf_day = group_per_frequency(df_train_transf_extra)\n",
    "df_test_gearbox_day = group_per_frequency(df_test_gearbox_extra)\n",
    "df_test_gen_day = group_per_frequency(df_test_gen_extra)\n",
    "df_test_gen_bear_day = group_per_frequency(df_test_gen_bear_extra)\n",
    "df_test_hyd_day = group_per_frequency(df_test_hyd_extra)\n",
    "df_test_transf_day = group_per_frequency(df_test_transf_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "premium-coalition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T12:44:26.538443Z",
     "start_time": "2021-02-06T12:44:26.510290Z"
    }
   },
   "outputs": [],
   "source": [
    "#Standard scaler per Turbine\n",
    "def scale(df_train, df_test, scaler='StandardScaler'):\n",
    "    \n",
    "    X_train = df_train.drop(columns=['Timestamp', 'TTF', '60_days', '30_days', '10_days', 'Component'])\n",
    "    X_test = df_test.drop(columns=['Timestamp', 'TTF', '60_days', '30_days', '10_days', 'Component'])\n",
    "    \n",
    "    X_train1 = X_train.loc[X_train['Turbine_ID']=='T01']\n",
    "    X_test1 = X_test.loc[X_test['Turbine_ID']=='T01']\n",
    "    \n",
    "    X_train1 = X_train1.drop(columns='Turbine_ID')\n",
    "    X_test1 = X_test1.drop(columns='Turbine_ID')\n",
    "    \n",
    "    if scaler == 'MinMaxScaler':\n",
    "        sc = MinMaxScaler()\n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "    else:\n",
    "        sc = StandardScaler()\n",
    "        X_train1 = sc.fit_transform(X_train1)\n",
    "        X_test1 = sc.transform(X_test1)\n",
    "    \n",
    "    turbines = ['T06', 'T07', 'T09', 'T11']\n",
    "    for turbine in turbines:\n",
    "        X_train_ = X_train.loc[X_train['Turbine_ID']==turbine]\n",
    "        X_test_ = X_test.loc[X_test['Turbine_ID']==turbine]\n",
    "        \n",
    "        X_train_ = X_train_.drop(columns='Turbine_ID')\n",
    "        X_test_ = X_test_.drop(columns='Turbine_ID')\n",
    "        \n",
    "        if scaler == 'MinMaxScaler':\n",
    "            sc = MinMaxScaler()\n",
    "            X_train_ = sc.fit_transform(X_train_)\n",
    "            X_test_ = sc.transform(X_test_)\n",
    "        else:\n",
    "            sc = StandardScaler()\n",
    "            X_train_ = sc.fit_transform(X_train_)\n",
    "            X_test_ = sc.transform(X_test_)\n",
    "\n",
    "        X_train1 = np.concatenate((X_train1, X_train_))\n",
    "        X_test1 = np.concatenate((X_test1, X_test_))\n",
    "        \n",
    "    return X_train1, X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cubic-mills",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T12:44:40.262180Z",
     "start_time": "2021-02-06T12:44:27.106383Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test = scale(temp_train, temp_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
